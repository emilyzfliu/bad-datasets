{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PaGELRUJqHM7"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(0)\n",
        "import scipy.misc\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import os\n",
        "device_ids = [0]\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "import pickle\n",
        "import copy\n",
        "from numpy.random import choice\n",
        "from torch.distributions import Categorical\n",
        "import scipy.stats\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g89ktiOIQoR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/baseline_cats.zip"
      ],
      "metadata": {
        "id": "Jgv38bTvC1Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "gbHLm2Mdz5-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, size, mode):\n",
        "        super(Interpolate, self).__init__()\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.size = size\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def multi_acc(y_pred, y_test):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
        "\n",
        "    correct_pred = (y_pred_tags == y_test).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "\n",
        "    acc = acc * 100\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def oht_to_scalar(y_pred):\n",
        "    y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
        "    _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
        "\n",
        "    return y_pred_tags\n",
        "\n",
        "def latent_to_image(g_all, upsamplers, latents, return_upsampled_layers=False, use_style_latents=False,\n",
        "                    style_latents=None, process_out=True, return_stylegan_latent=False, dim=512, return_only_im=False):\n",
        "    '''Given a input latent code, generate corresponding image and concatenated feature maps'''\n",
        "\n",
        "    # assert (len(latents) == 1)  # for GPU memory constraints\n",
        "    if not use_style_latents:\n",
        "        # generate style_latents from latents\n",
        "        style_latents = g_all.module.truncation(g_all.module.g_mapping(latents))\n",
        "        style_latents = style_latents.clone()  # make different layers non-alias\n",
        "\n",
        "    else:\n",
        "        style_latents = latents\n",
        "\n",
        "        # style_latents = latents\n",
        "    if return_stylegan_latent:\n",
        "\n",
        "        return  style_latents\n",
        "    img_list, affine_layers = g_all.module.g_synthesis(style_latents)\n",
        "\n",
        "    if return_only_im:\n",
        "        if process_out:\n",
        "            if img_list.shape[-2] > 512:\n",
        "                img_list = upsamplers[-1](img_list)\n",
        "\n",
        "            img_list = img_list.cpu().detach().numpy()\n",
        "            img_list = process_image(img_list)\n",
        "            img_list = np.transpose(img_list, (0, 2, 3, 1)).astype(np.uint8)\n",
        "        return img_list, style_latents\n",
        "\n",
        "    number_feautre = 0\n",
        "\n",
        "    for item in affine_layers:\n",
        "        number_feautre += item.shape[1]\n",
        "\n",
        "\n",
        "    affine_layers_upsamples = torch.FloatTensor(1, number_feautre, dim, dim)\n",
        "    if torch.cuda.is_available():\n",
        "        affine_layers_upsamples = affine_layers_upsamples.cuda()\n",
        "    if return_upsampled_layers:\n",
        "\n",
        "        start_channel_index = 0\n",
        "        for i in range(len(affine_layers)):\n",
        "            len_channel = affine_layers[i].shape[1]\n",
        "            affine_layers_upsamples[:, start_channel_index:start_channel_index + len_channel] = upsamplers[i](\n",
        "                affine_layers[i])\n",
        "            start_channel_index += len_channel\n",
        "\n",
        "    if img_list.shape[-2] != 512:\n",
        "        img_list = upsamplers[-1](img_list)\n",
        "\n",
        "    if process_out:\n",
        "        img_list = img_list.cpu().detach().numpy()\n",
        "        img_list = process_image(img_list)\n",
        "        img_list = np.transpose(img_list, (0, 2, 3, 1)).astype(np.uint8)\n",
        "        # print('start_channel_index',start_channel_index)\n",
        "\n",
        "\n",
        "    return img_list, affine_layers_upsamples\n",
        "\n",
        "\n",
        "def process_image(images):\n",
        "    drange = [-1, 1]\n",
        "    scale = 255 / (drange[1] - drange[0])\n",
        "    images = images * scale + (0.5 - drange[0] * scale)\n",
        "\n",
        "    images = images.astype(int)\n",
        "    images[images > 255] = 255\n",
        "    images[images < 0] = 0\n",
        "\n",
        "    return images.astype(int)\n",
        "\n",
        "def colorize_mask(mask, palette):\n",
        "    # mask: numpy array of the mask\n",
        "\n",
        "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
        "    new_mask.putpalette(palette)\n",
        "    return np.array(new_mask.convert('RGB'))\n",
        "\n",
        "\n",
        "def get_label_stas(data_loader):\n",
        "    count_dict = {}\n",
        "    for i in range(data_loader.__len__()):\n",
        "        x, y = data_loader.__getitem__(i)\n",
        "        if int(y.item()) not in count_dict:\n",
        "            count_dict[int(y.item())] = 1\n",
        "        else:\n",
        "            count_dict[int(y.item())] += 1\n",
        "\n",
        "    return count_dict\n"
      ],
      "metadata": {
        "id": "NPCcLvszq-q1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StyleGAN nets"
      ],
      "metadata": {
        "id": "XTfZl8aUz-Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLinear(nn.Module):\n",
        "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, gain=2 ** (0.5), use_wscale=False, lrmul=1, bias=True):\n",
        "        super().__init__()\n",
        "        he_std = gain * input_size ** (-0.5)  # He init\n",
        "        # Equalized learning rate and custom learning rate multiplier.\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "        return F.linear(x, self.weight * self.w_mul, bias)\n",
        "\n",
        "\n",
        "class MyConv2d(nn.Module):\n",
        "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, gain=2 ** (0.5), use_wscale=False,\n",
        "                 lrmul=1, bias=True,\n",
        "                 intermediate=None, upscale=False, downscale=False):\n",
        "        super().__init__()\n",
        "        if upscale:\n",
        "            self.upscale = Upscale2d()\n",
        "        else:\n",
        "            self.upscale = None\n",
        "        if downscale:\n",
        "            self.downscale = Downscale2d()\n",
        "        else:\n",
        "            self.downscale = None\n",
        "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5)  # He init\n",
        "        self.kernel_size = kernel_size\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(\n",
        "            torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.intermediate = intermediate\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "\n",
        "        have_convolution = False\n",
        "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
        "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
        "            # this really needs to be cleaned up and go into the conv...\n",
        "            w = self.weight * self.w_mul\n",
        "            w = w.permute(1, 0, 2, 3)\n",
        "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
        "            w = F.pad(w, (1, 1, 1, 1))\n",
        "            w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
        "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1) - 1) // 2)\n",
        "            have_convolution = True\n",
        "        elif self.upscale is not None:\n",
        "            x = self.upscale(x)\n",
        "\n",
        "        downscale = self.downscale\n",
        "        intermediate = self.intermediate\n",
        "        if downscale is not None and min(x.shape[2:]) >= 128:\n",
        "            w = self.weight * self.w_mul\n",
        "            w = F.pad(w, (1, 1, 1, 1))\n",
        "            # in contrast to upscale, this is a mean...\n",
        "            w = (w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]) * 0.25  # avg_pool?\n",
        "            x = F.conv2d(x, w, stride=2, padding=(w.size(-1) - 1) // 2)\n",
        "            have_convolution = True\n",
        "            downscale = None\n",
        "        elif downscale is not None:\n",
        "            assert intermediate is None\n",
        "            intermediate = downscale\n",
        "\n",
        "        if not have_convolution and intermediate is None:\n",
        "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size // 2)\n",
        "        elif not have_convolution:\n",
        "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size // 2)\n",
        "\n",
        "        if intermediate is not None:\n",
        "            x = intermediate(x)\n",
        "\n",
        "        if bias is not None:\n",
        "            x = x + bias.view(1, -1, 1, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoiseLayer(nn.Module):\n",
        "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(channels))\n",
        "        self.noise = None\n",
        "\n",
        "    def forward(self, x, noise=None):\n",
        "        if noise is None and self.noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
        "        elif noise is None:\n",
        "            # here is a little trick: if you get all the noiselayers and set each\n",
        "            # modules .noise attribute, you can have pre-defined noise.\n",
        "            # Very useful for analysis\n",
        "            noise = self.noise\n",
        "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
        "        return x\n",
        "\n",
        "\n",
        "class StyleMod(nn.Module):\n",
        "    def __init__(self, latent_size, channels, use_wscale):\n",
        "        super(StyleMod, self).__init__()\n",
        "        self.lin = MyLinear(latent_size,\n",
        "                            channels * 2,\n",
        "                            gain=1.0, use_wscale=use_wscale)\n",
        "        self.x_param_backup = None\n",
        "\n",
        "    def forward(self, x, latent, latent_after_trans=None):\n",
        "        if x is not None:\n",
        "            if latent_after_trans is None:\n",
        "                style = self.lin(latent)  # style => [batch_size, n_channels*2]\n",
        "                shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
        "                style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
        "            else:\n",
        "                style = latent_after_trans\n",
        "\n",
        "            self.x_param_backup = [x.size(1), x.dim()]\n",
        "            x = x * (style[:, 0] + 1.) + style[:, 1]\n",
        "            return x\n",
        "\n",
        "        else:\n",
        "            if self.x_param_backup is None:\n",
        "                print('error: have intialize shape yet')\n",
        "            # print('Generating latent_after_trans:')\n",
        "            style = self.lin(latent)  # style => [batch_size, n_channels*2]\n",
        "            shape = [-1, 2, self.x_param_backup[0]] + (self.x_param_backup[1] - 2) * [1]\n",
        "            style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
        "            return style\n",
        "\n",
        "\n",
        "class PixelNormLayer(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "# Upscale and blur layers\n",
        "\n",
        "\n",
        "class BlurLayer(nn.Module):\n",
        "    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n",
        "        super(BlurLayer, self).__init__()\n",
        "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
        "        kernel = kernel[:, None] * kernel[None, :]\n",
        "        kernel = kernel[None, None]\n",
        "        if normalize:\n",
        "            kernel = kernel / kernel.sum()\n",
        "        if flip:\n",
        "            kernel = kernel[:, :, ::-1, ::-1]\n",
        "        self.register_buffer('kernel', kernel)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # expand kernel channels\n",
        "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
        "        x = F.conv2d(\n",
        "            x,\n",
        "            kernel,\n",
        "            stride=self.stride,\n",
        "            padding=int((self.kernel.size(2) - 1) / 2),\n",
        "            groups=x.size(1)\n",
        "        )\n",
        "        return x\n",
        "\n",
        "\n",
        "def upscale2d(x, factor=2, gain=1):\n",
        "    assert x.dim() == 4\n",
        "    if gain != 1:\n",
        "        x = x * gain\n",
        "    if factor != 1:\n",
        "        shape = x.shape\n",
        "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
        "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
        "    return x\n",
        "\n",
        "\n",
        "class Upscale2d(nn.Module):\n",
        "    def __init__(self, factor=2, gain=1):\n",
        "        super().__init__()\n",
        "        assert isinstance(factor, int) and factor >= 1\n",
        "        self.gain = gain\n",
        "        self.factor = factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return upscale2d(x, factor=self.factor, gain=self.gain)\n",
        "\n",
        "\n",
        "class G_mapping(nn.Sequential):\n",
        "    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        layers = [\n",
        "            ('pixel_norm', PixelNormLayer()),\n",
        "            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense0_act', act),\n",
        "            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense1_act', act),\n",
        "            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense2_act', act),\n",
        "            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense3_act', act),\n",
        "            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense4_act', act),\n",
        "            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense5_act', act),\n",
        "            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense6_act', act),\n",
        "            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense7_act', act)\n",
        "        ]\n",
        "        super().__init__(OrderedDict(layers))\n",
        "\n",
        "\n",
        "\n",
        "    def make_mean_latent(self, n_latent):\n",
        "        latent_in = torch.randn(\n",
        "            n_latent, 512\n",
        "        ).cuda()\n",
        "        mean_latent = super().forward(latent_in).mean(0, keepdim=True)\n",
        "        mean_latent = mean_latent.unsqueeze(1).expand(-1, 18, -1)\n",
        "        return mean_latent\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        # Broadcast\n",
        "        x = x.unsqueeze(1).expand(-1, 18, -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Truncation(nn.Module):\n",
        "    def __init__(self, avg_latent, device, max_layer=8, threshold=0.7):\n",
        "        super().__init__()\n",
        "        self.max_layer = max_layer\n",
        "        self.threshold = threshold\n",
        "        self.avg_latent = avg_latent\n",
        "        self.device = device\n",
        "        # self.register_buffer('avg_latent', avg_latent)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 3\n",
        "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
        "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1).to(self.device)\n",
        "        return torch.where(do_trunc, interp, x)\n",
        "\n",
        "\n",
        "class LayerEpilogue(nn.Module):\n",
        "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
        "\n",
        "    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles,\n",
        "                 activation_layer):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        if use_noise:\n",
        "            layers.append(('noise', NoiseLayer(channels)))\n",
        "        layers.append(('activation', activation_layer))\n",
        "        if use_pixel_norm:\n",
        "            layers.append(('pixel_norm', PixelNorm()))\n",
        "        if use_instance_norm:\n",
        "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
        "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
        "        if use_styles:\n",
        "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
        "        else:\n",
        "            self.style_mod = None\n",
        "\n",
        "    def forward(self, x, dlatents_in_slice=None, latent_after_trans=None):\n",
        "        x = self.top_epi(x)\n",
        "        if self.style_mod is not None:\n",
        "            if latent_after_trans is None:\n",
        "                x = self.style_mod(x, dlatents_in_slice)\n",
        "            else:\n",
        "                x = self.style_mod(x, dlatents_in_slice, latent_after_trans)\n",
        "        else:\n",
        "            assert dlatents_in_slice is None\n",
        "        return x\n",
        "\n",
        "\n",
        "class InputBlock(nn.Module):\n",
        "    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm,\n",
        "                 use_instance_norm, use_styles, activation_layer):\n",
        "        super().__init__()\n",
        "        self.const_input_layer = const_input_layer\n",
        "        self.nf = nf\n",
        "        if self.const_input_layer:\n",
        "            # called 'const' in tf\n",
        "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
        "            self.bias = nn.Parameter(torch.ones(nf))\n",
        "        else:\n",
        "            self.dense = MyLinear(dlatent_size, nf * 16, gain=gain / 4,\n",
        "                                  use_wscale=use_wscale)  # tweak gain to match the official implementation of Progressing GAN\n",
        "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
        "                                  use_styles, activation_layer)\n",
        "        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
        "                                  use_styles, activation_layer)\n",
        "\n",
        "    def forward(self, dlatents_in_range, latent_after_trans=None):\n",
        "        batch_size = dlatents_in_range.size(0)\n",
        "        if self.const_input_layer:\n",
        "            x = self.const.expand(batch_size, -1, -1, -1)\n",
        "            x = x + self.bias.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
        "\n",
        "        if latent_after_trans is None:\n",
        "            x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        else:\n",
        "            x = self.epi1(x, dlatents_in_range[:, 0], latent_after_trans[0])  # latent_after_trans is a list\n",
        "\n",
        "        x = self.conv(x)\n",
        "\n",
        "        if latent_after_trans is None:\n",
        "            x1 = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        else:\n",
        "            x1 = self.epi2(x, dlatents_in_range[:, 1], latent_after_trans[1])\n",
        "\n",
        "        return x1, x\n",
        "\n",
        "\n",
        "class GSynthesisBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise,\n",
        "                 use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        # 2**res x 2**res # res = 3..resolution_log2\n",
        "        super().__init__()\n",
        "        if blur_filter:\n",
        "            blur = BlurLayer(blur_filter)\n",
        "        else:\n",
        "            blur = None\n",
        "        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
        "                                 intermediate=blur, upscale=True)\n",
        "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
        "                                  use_styles, activation_layer)\n",
        "        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm,\n",
        "                                  use_styles, activation_layer)\n",
        "\n",
        "    def forward(self, x, dlatents_in_range, latent_after_trans=None):\n",
        "        x = self.conv0_up(x)\n",
        "\n",
        "        if latent_after_trans is None:\n",
        "            x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        else:\n",
        "            x = self.epi1(x, dlatents_in_range[:, 0], latent_after_trans[0])  # latent_after_trans is a list\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if latent_after_trans is None:\n",
        "            x1 = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        else:\n",
        "            x1 = self.epi2(x, dlatents_in_range[:, 1], latent_after_trans[1])\n",
        "        return x1, x\n",
        "\n",
        "\n",
        "class SegSynthesisBlock(nn.Module):\n",
        "    def __init__(self, prev_channel, current_channel, single_in=False):\n",
        "        super().__init__()\n",
        "        self.single_in = single_in\n",
        "        # self.in_conv = nn.Sequential(\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(current_channel, current_channel, 3, 1, 1),\n",
        "        #     nn.BatchNorm2d(current_channel),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv2d(current_channel, current_channel, 1),\n",
        "        #     nn.BatchNorm2d(current_channel)\n",
        "        # )\n",
        "\n",
        "        if not single_in:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
        "\n",
        "            self.out_conv1 = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(current_channel + prev_channel, current_channel, 1, 1, 0),\n",
        "                nn.BatchNorm2d(current_channel)\n",
        "            )\n",
        "\n",
        "        self.out_conv2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(current_channel + current_channel, current_channel, 1, 1, 0),\n",
        "            nn.BatchNorm2d(current_channel)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_curr, x_curr2, x_prev=None):\n",
        "\n",
        "        # x_curr = self.in_conv(x_curr)\n",
        "\n",
        "        if self.single_in:\n",
        "            x_middle = x_curr\n",
        "        else:\n",
        "            x_prev = self.up(x_prev)\n",
        "            x_concat = torch.cat([x_curr, x_prev], 1)\n",
        "\n",
        "            x_middle = self.out_conv1(x_concat)\n",
        "\n",
        "            x_middle = x_middle + x_curr\n",
        "\n",
        "        x_concat2 = torch.cat([x_curr2, x_middle], 1)\n",
        "        x_out = self.out_conv2(x_concat2)\n",
        "        x_out = x_out + x_curr2\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class G_synthesis(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dlatent_size=512,  # Disentangled latent (W) dimensionality.\n",
        "                 num_channels=3,  # Number of output color channels.\n",
        "                 resolution=512,  # Output resolution.\n",
        "                 fmap_base=8192,  # Overall multiplier for the number of feature maps.\n",
        "                 fmap_decay=1.0,  # log2 feature map reduction when doubling the resolution.\n",
        "                 fmap_max=512,  # Maximum number of feature maps in any layer.\n",
        "                 use_styles=True,  # Enable style inputs?\n",
        "                 const_input_layer=True,  # First layer is a learned constant?\n",
        "                 use_noise=True,  # Enable noise inputs?\n",
        "                 randomize_noise=True,\n",
        "                 # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
        "                 nonlinearity='lrelu',  # Activation function: 'relu', 'lrelu'\n",
        "                 use_wscale=True,  # Enable equalized learning rate?\n",
        "                 use_pixel_norm=False,  # Enable pixelwise feature vector normalization?\n",
        "                 use_instance_norm=True,  # Enable instance normalization?\n",
        "                 dtype=torch.float32,  # Data type to use for activations and outputs.\n",
        "                 fused_scale='auto',\n",
        "                 # True = fused convolution + scaling, False = separate ops, 'auto' = decide automatically.\n",
        "                 blur_filter=[1, 2, 1],  # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "                 structure='auto',\n",
        "                 # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient, 'auto' = select automatically.\n",
        "                 is_template_graph=False,\n",
        "                 # True = template graph constructed by the Network class, False = actual evaluation.\n",
        "                 force_clean_graph=False,\n",
        "                 # True = construct a clean graph that looks nice in TensorBoard, False = default behavior.\n",
        "                 seg_branch=False\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "        self.dlatent_size = dlatent_size\n",
        "        self.seg_branch = seg_branch\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
        "        if is_template_graph: force_clean_graph = True\n",
        "        if force_clean_graph: randomize_noise = False\n",
        "        if structure == 'auto': structure = 'linear' if force_clean_graph else 'recursive'\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        num_layers = resolution_log2 * 2 - 2\n",
        "        num_styles = num_layers if use_styles else 1\n",
        "        torgbs = []\n",
        "        blocks = []\n",
        "        if self.seg_branch:\n",
        "            seg_block = []\n",
        "        for res in range(2, resolution_log2 + 1):\n",
        "\n",
        "            channels = nf(res - 1)\n",
        "            name = '{s}x{s}'.format(s=2 ** res)\n",
        "            if res == 2:\n",
        "                blocks.append((name,\n",
        "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
        "                                          use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                blocks.append((name,\n",
        "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale,\n",
        "                                               use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "\n",
        "                if self.seg_branch:\n",
        "\n",
        "                    name = '{s}x{s}_seg'.format(s=2 ** res)\n",
        "\n",
        "                    if len(seg_block) == 0:\n",
        "                        seg_block.append((name,\n",
        "                                          SegSynthesisBlock(last_channels, channels, single_in=True)))\n",
        "                    else:\n",
        "                        seg_block.append((name,\n",
        "                                          SegSynthesisBlock(last_channels, channels)))\n",
        "\n",
        "            last_channels = channels\n",
        "        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n",
        "\n",
        "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
        "        if self.seg_branch:\n",
        "            seg_block.append((\"seg_out\", nn.Conv2d(channels, 34, 1)))\n",
        "            self.seg_block = nn.ModuleDict(OrderedDict(seg_block))\n",
        "\n",
        "    def forward(self, dlatents_in, latent_after_trans=None):\n",
        "        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n",
        "        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n",
        "        batch_size = dlatents_in.size(0)\n",
        "        result_list = []\n",
        "\n",
        "        if self.seg_branch:\n",
        "            seg_branch_feature = None\n",
        "        for i, m in enumerate(self.blocks.values()):\n",
        "            if i == 0:\n",
        "                if latent_after_trans is None:\n",
        "                    x, x2 = m(dlatents_in[:, 2 * i:2 * i + 2])\n",
        "                else:\n",
        "                    x, x2 = m(dlatents_in[:, 2 * i:2 * i + 2], latent_after_trans[2 * i:2 * i + 2])\n",
        "            else:\n",
        "\n",
        "                if latent_after_trans is None:\n",
        "                    x, x2 = m(x, dlatents_in[:, 2 * i:2 * i + 2])\n",
        "                else:\n",
        "                    x, x2 = m(x, dlatents_in[:, 2 * i:2 * i + 2],\n",
        "                              latent_after_trans[2 * i:2 * i + 2])  # latent_after_trans is a tensor list\n",
        "\n",
        "                if self.seg_branch:\n",
        "\n",
        "                    name = '{s}x{s}_seg'.format(s=2 ** (i + 2))\n",
        "\n",
        "                    curr_seg_block = self.seg_block[name]\n",
        "                    if seg_branch_feature is None:\n",
        "                        seg_branch_feature = curr_seg_block(x2, x)\n",
        "                    else:\n",
        "                        seg_branch_feature = curr_seg_block(x2, x, x_prev=seg_branch_feature)\n",
        "\n",
        "            result_list.append(x)\n",
        "            result_list.append(x2)\n",
        "        rgb = self.torgb(x)\n",
        "        if self.seg_branch:\n",
        "            seg = self.seg_block[\"seg_out\"](seg_branch_feature)\n",
        "            return rgb, seg, result_list\n",
        "        return rgb, result_list\n",
        "\n",
        "\n",
        "#### define discriminator\n",
        "\n",
        "class StddevLayer(nn.Module):\n",
        "    def __init__(self, group_size=4, num_new_features=1):\n",
        "        super().__init__()\n",
        "        self.group_size = 4\n",
        "        self.num_new_features = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        group_size = min(self.group_size, b)\n",
        "        y = x.reshape([group_size, -1, self.num_new_features,\n",
        "                       c // self.num_new_features, h, w])\n",
        "        y = y - y.mean(0, keepdim=True)\n",
        "        y = (y ** 2).mean(0, keepdim=True)\n",
        "        y = (y + 1e-8) ** 0.5\n",
        "        y = y.mean([3, 4, 5], keepdim=True).squeeze(3)  # don't keep the meaned-out channels\n",
        "        y = y.expand(group_size, -1, -1, h, w).clone().reshape(b, self.num_new_features, h, w)\n",
        "        z = torch.cat([x, y], dim=1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class Downscale2d(nn.Module):\n",
        "    def __init__(self, factor=2, gain=1):\n",
        "        super().__init__()\n",
        "        assert isinstance(factor, int) and factor >= 1\n",
        "        self.factor = factor\n",
        "        self.gain = gain\n",
        "        if factor == 2:\n",
        "            f = [np.sqrt(gain) / factor] * factor\n",
        "            self.blur = BlurLayer(kernel=f, normalize=False, stride=factor)\n",
        "        else:\n",
        "            self.blur = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 4\n",
        "        # 2x2, float32 => downscale using _blur2d().\n",
        "        if self.blur is not None and x.dtype == torch.float32:\n",
        "            return self.blur(x)\n",
        "\n",
        "        # Apply gain.\n",
        "        if self.gain != 1:\n",
        "            x = x * self.gain\n",
        "\n",
        "        # No-op => early exit.\n",
        "        if factor == 1:\n",
        "            return x\n",
        "\n",
        "        # Large factor => downscale using tf.nn.avg_pool().\n",
        "        # NOTE: Requires tf_config['graph_options.place_pruned_graph']=True to work.\n",
        "        return F.avg_pool2d(x, self.factor)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, gain, use_wscale, activation_layer):\n",
        "        super().__init__(OrderedDict([\n",
        "            ('conv0', MyConv2d(in_channels, in_channels, 3, gain=gain, use_wscale=use_wscale)),\n",
        "            # out channels nf(res-1)\n",
        "            ('act0', activation_layer),\n",
        "            ('blur', BlurLayer()),\n",
        "            ('conv1_down', MyConv2d(in_channels, out_channels, 3, gain=gain, use_wscale=use_wscale, downscale=True)),\n",
        "            ('act1', activation_layer)]))\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), *self.shape)\n",
        "\n",
        "\n",
        "class DiscriminatorTop(nn.Sequential):\n",
        "    def __init__(self, mbstd_group_size, mbstd_num_features, in_channels, intermediate_channels, gain, use_wscale,\n",
        "                 activation_layer, resolution=4, in_channels2=None, output_features=1, last_gain=1):\n",
        "        layers = []\n",
        "        if mbstd_group_size > 1:\n",
        "            layers.append(('stddev_layer', StddevLayer(mbstd_group_size, mbstd_num_features)))\n",
        "        if in_channels2 is None:\n",
        "            in_channels2 = in_channels\n",
        "        layers.append(\n",
        "            ('conv', MyConv2d(in_channels + mbstd_num_features, in_channels2, 3, gain=gain, use_wscale=use_wscale)))\n",
        "        layers.append(('act0', activation_layer))\n",
        "        layers.append(('view', View(-1)))\n",
        "        layers.append(('dense0', MyLinear(in_channels2 * resolution * resolution, intermediate_channels, gain=gain,\n",
        "                                          use_wscale=use_wscale)))\n",
        "        layers.append(('act1', activation_layer))\n",
        "        layers.append(\n",
        "            ('dense1', MyLinear(intermediate_channels, output_features, gain=last_gain, use_wscale=use_wscale)))\n",
        "        super().__init__(OrderedDict(layers))\n",
        "\n",
        "\n",
        "class D_basic(nn.Sequential):\n",
        "\n",
        "    def __init__(self,\n",
        "                 # images_in,                          # First input: Images [minibatch, channel, height, width].\n",
        "                 # labels_in,                          # Second input: Labels [minibatch, label_size].\n",
        "                 num_channels=3,  # Number of input color channels. Overridden based on dataloader.\n",
        "                 resolution=512,  # Input resolution. Overridden based on dataloader.\n",
        "                 fmap_base=8192,  # Overall multiplier for the number of feature maps.\n",
        "                 fmap_decay=1.0,  # log2 feature map reduction when doubling the resolution.\n",
        "                 fmap_max=512,  # Maximum number of feature maps in any layer.\n",
        "                 nonlinearity='lrelu',  # Activation function: 'relu', 'lrelu',\n",
        "                 use_wscale=True,  # Enable equalized learning rate?\n",
        "                 mbstd_group_size=4,  # Group size for the minibatch standard deviation layer, 0 = disable.\n",
        "                 mbstd_num_features=1,  # Number of features for the minibatch standard deviation layer.\n",
        "                 # blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "                 ):\n",
        "        self.mbstd_group_size = 4\n",
        "        self.mbstd_num_features = 1\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2 ** resolution_log2 and resolution >= 4\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        self.gain = gain\n",
        "        self.use_wscale = use_wscale\n",
        "        super().__init__(OrderedDict([\n",
        "                                         ('fromrgb', MyConv2d(num_channels, nf(resolution_log2 - 1), 1, gain=gain,\n",
        "                                                              use_wscale=use_wscale)),\n",
        "                                         ('act', act)]\n",
        "                                     + [('{s}x{s}'.format(s=2 ** res),\n",
        "                                         DiscriminatorBlock(nf(res - 1), nf(res - 2), gain=gain, use_wscale=use_wscale,\n",
        "                                                            activation_layer=act)) for res in\n",
        "                                        range(resolution_log2, 2, -1)]\n",
        "                                     + [('4x4',\n",
        "                                         DiscriminatorTop(mbstd_group_size, mbstd_num_features, nf(2), nf(2), gain=gain,\n",
        "                                                          use_wscale=use_wscale, activation_layer=act))]))"
      ],
      "metadata": {
        "id": "P5QFM-G1q4vw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "-aTSTGRO0Jbm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tgOHxqLmqHM9"
      },
      "outputs": [],
      "source": [
        "class trainData(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "        print('initializing train data')\n",
        "        self.X_data = X_data\n",
        "        print('x_data set')\n",
        "        self.y_data = y_data\n",
        "        print('y_data set')\n",
        "        print('trainData initialized')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "class pixel_classifier(nn.Module):\n",
        "    def __init__(self, numpy_class, dim):\n",
        "        super(pixel_classifier, self).__init__()\n",
        "        if numpy_class < 32:\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(dim, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(num_features=128),\n",
        "                nn.Linear(128, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(num_features=32),\n",
        "                nn.Linear(32, numpy_class),\n",
        "                # nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(dim, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(num_features=256),\n",
        "                nn.Linear(256, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(num_features=128),\n",
        "                nn.Linear(128, numpy_class),\n",
        "                # nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def init_weights(self, init_type='normal', gain=0.02):\n",
        "        '''\n",
        "        initialize network's weights\n",
        "        init_type: normal | xavier | kaiming | orthogonal\n",
        "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/\n",
        "        9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
        "        '''\n",
        "\n",
        "        def init_func(m):\n",
        "            classname = m.__class__.__name__\n",
        "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "                if init_type == 'normal':\n",
        "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
        "                elif init_type == 'xavier':\n",
        "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "                elif init_type == 'kaiming':\n",
        "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "                elif init_type == 'orthogonal':\n",
        "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
        "\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "            elif classname.find('BatchNorm2d') != -1:\n",
        "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        self.apply(init_func)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare_stylegan"
      ],
      "metadata": {
        "id": "bA_tzpBx0Ldp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "45wLMb1FqHM_"
      },
      "outputs": [],
      "source": [
        "def prepare_stylegan(args):\n",
        "    print('preparing stylegan...')\n",
        "    if args['stylegan_ver'] == \"1\":\n",
        "        if args['category'] == \"car\":\n",
        "            resolution = 512\n",
        "            max_layer = 8\n",
        "        elif  args['category'] == \"face\":\n",
        "            resolution = 1024\n",
        "            max_layer = 8\n",
        "        elif args['category'] == \"bedroom\":\n",
        "            resolution = 256\n",
        "            max_layer = 7\n",
        "        elif args['category'] == \"cat\":\n",
        "            resolution = 256\n",
        "            max_layer = 7\n",
        "        else:\n",
        "            assert \"Not implementated!\"\n",
        "\n",
        "        avg_latent = np.load(args['average_latent'])\n",
        "        avg_latent = torch.from_numpy(avg_latent).type(torch.FloatTensor).to(device)\n",
        "\n",
        "        g_all = nn.Sequential(OrderedDict([\n",
        "            ('g_mapping', G_mapping()),\n",
        "            ('truncation', Truncation(avg_latent,max_layer=max_layer, device=device, threshold=0.7)),\n",
        "            ('g_synthesis', G_synthesis( resolution=resolution))\n",
        "        ]))\n",
        "\n",
        "        g_all.load_state_dict(torch.load(args['stylegan_checkpoint'], map_location=device))\n",
        "        g_all.eval()\n",
        "        g_all = nn.DataParallel(g_all, device_ids=device_ids)\n",
        "        if torch.cuda.is_available():\n",
        "            g_all = g_all.cuda()\n",
        "\n",
        "    else:\n",
        "        assert \"Not implementated error\"\n",
        "\n",
        "    res  = args['dim'][1]\n",
        "    mode = args['upsample_mode']\n",
        "    upsamplers = [nn.Upsample(scale_factor=res / 4, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 4, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 8, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 8, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 16, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 16, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 32, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 32, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 64, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 64, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 128, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 128, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 256, mode=mode),\n",
        "                  nn.Upsample(scale_factor=res / 256, mode=mode)\n",
        "                  ]\n",
        "\n",
        "    if resolution > 256:\n",
        "        upsamplers.append(nn.Upsample(scale_factor=res / 512, mode=mode))\n",
        "        upsamplers.append(nn.Upsample(scale_factor=res / 512, mode=mode))\n",
        "\n",
        "    if resolution > 512:\n",
        "\n",
        "        upsamplers.append(Interpolate(res, 'bilinear'))\n",
        "        upsamplers.append(Interpolate(res, 'bilinear'))\n",
        "\n",
        "    return g_all, avg_latent, upsamplers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "car_20_palette =[ 255,  255,  255, # 0 background\n",
        "  238,  229,  102,# 1 back_bumper\n",
        "  0, 0, 0,# 2 bumper\n",
        "  124,  99 , 34, # 3 car\n",
        "  193 , 127,  15,# 4 car_lights\n",
        "  248  ,213 , 42, # 5 door\n",
        "  220  ,147 , 77, # 6 fender\n",
        "  99 , 83  , 3, # 7 grilles\n",
        "  116 , 116 , 138,  # 8 handles\n",
        "  200  ,226 , 37, # 9 hoods\n",
        "  225 , 184 , 161, # 10 licensePlate\n",
        "  142 , 172  ,248, # 11 mirror\n",
        "  153 , 112 , 146, # 12 roof\n",
        "  38  ,112 , 254, # 13 running_boards\n",
        "  229 , 30  ,141, # 14 tailLight\n",
        "  52 , 83  ,84, # 15 tire\n",
        "  194 , 87 , 125, # 16 trunk_lids\n",
        "  225,  96  ,18,  # 17 wheelhub\n",
        "  31 , 102 , 211, # 18 window\n",
        "  104 , 131 , 101# 19 windshield\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "face_palette = [  1.0000,  1.0000 , 1.0000,\n",
        "              0.4420,  0.5100 , 0.4234,\n",
        "              0.8562,  0.9537 , 0.3188,\n",
        "              0.2405,  0.4699 , 0.9918,\n",
        "              0.8434,  0.9329  ,0.7544,\n",
        "              0.3748,  0.7917 , 0.3256,\n",
        "              0.0190,  0.4943 , 0.3782,\n",
        "              0.7461 , 0.0137 , 0.5684,\n",
        "              0.1644,  0.2402 , 0.7324,\n",
        "              0.0200 , 0.4379 , 0.4100,\n",
        "              0.5853 , 0.8880 , 0.6137,\n",
        "              0.7991 , 0.9132 , 0.9720,\n",
        "              0.6816 , 0.6237  ,0.8562,\n",
        "              0.9981 , 0.4692 , 0.3849,\n",
        "              0.5351 , 0.8242 , 0.2731,\n",
        "              0.1747 , 0.3626 , 0.8345,\n",
        "              0.5323 , 0.6668 , 0.4922,\n",
        "              0.2122 , 0.3483 , 0.4707,\n",
        "              0.6844,  0.1238 , 0.1452,\n",
        "              0.3882 , 0.4664 , 0.1003,\n",
        "              0.2296,  0.0401 , 0.3030,\n",
        "              0.5751 , 0.5467 , 0.9835,\n",
        "              0.1308 , 0.9628,  0.0777,\n",
        "              0.2849  ,0.1846 , 0.2625,\n",
        "              0.9764 , 0.9420 , 0.6628,\n",
        "              0.3893 , 0.4456 , 0.6433,\n",
        "              0.8705 , 0.3957 , 0.0963,\n",
        "              0.6117 , 0.9702 , 0.0247,\n",
        "              0.3668 , 0.6694 , 0.3117,\n",
        "              0.6451 , 0.7302,  0.9542,\n",
        "              0.6171 , 0.1097,  0.9053,\n",
        "              0.3377 , 0.4950,  0.7284,\n",
        "              0.1655,  0.9254,  0.6557,\n",
        "              0.9450  ,0.6721,  0.6162]\n",
        "\n",
        "face_palette = [int(item * 255) for item in face_palette]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "car_12_palette =[ 255,  255,  255, # 0 background\n",
        "         124,  99 , 34, # 3 car\n",
        "         193 , 127,  15,# 4 car_lights\n",
        "         229 , 30  ,141, # 14 tailLight\n",
        "        225 , 184 , 161, # 10 licensePlate\n",
        "        104 , 131 , 101,# 19 windshield\n",
        "        52 , 83  ,84, # 15 tire\n",
        "        248  ,213 , 42, # 5 door\n",
        "         116 , 116 , 138,  # 8 handles\n",
        "           225,  96  ,18,  # 17 wheelhub\n",
        "         31 , 102 , 211, # 18 window\n",
        "         142 , 172  ,248, # 11 mirror\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "car_32_palette =[ 255,  255,  255,\n",
        "  238,  229,  102,\n",
        "  0, 0, 0,\n",
        "  124,  99 , 34,\n",
        "  193 , 127,  15,\n",
        "  106,  177,  21,\n",
        "  248  ,213 , 42,\n",
        "  252 , 155,  83,\n",
        "  220  ,147 , 77,\n",
        "  99 , 83  , 3,\n",
        "  116 , 116 , 138,\n",
        "  63  ,182 , 24,\n",
        "  200  ,226 , 37,\n",
        "  225 , 184 , 161,\n",
        "  233 ,  5  ,219,\n",
        "  142 , 172  ,248,\n",
        "  153 , 112 , 146,\n",
        "  38  ,112 , 254,\n",
        "  229 , 30  ,141,\n",
        "  115  ,208 , 131,\n",
        "  52 , 83  ,84,\n",
        "  229 , 63 , 110,\n",
        "  194 , 87 , 125,\n",
        "  225,  96  ,18,\n",
        "  73  ,139,  226,\n",
        "  172 , 143 , 16,\n",
        "  169 , 101 , 111,\n",
        "  31 , 102 , 211,\n",
        "  104 , 131 , 101,\n",
        "  70  ,168  ,156,\n",
        "  183 , 242 , 209,\n",
        "  72  ,184 , 226]\n",
        "\n",
        "bedroom_palette =[ 255,  255,  255,\n",
        "  238,  229,  102,\n",
        "  255, 72, 69,\n",
        "  124,  99 , 34,\n",
        "  193 , 127,  15,\n",
        "  106,  177,  21,\n",
        "  248  ,213 , 42,\n",
        "  252 , 155,  83,\n",
        "  220  ,147 , 77,\n",
        "  99 , 83  , 3,\n",
        "  116 , 116 , 138,\n",
        "  63  ,182 , 24,\n",
        "  200  ,226 , 37,\n",
        "  225 , 184 , 161,\n",
        "  233 ,  5  ,219,\n",
        "  142 , 172  ,248,\n",
        "  153 , 112 , 146,\n",
        "  38  ,112 , 254,\n",
        "  229 , 30  ,141,\n",
        "   238, 229, 12,\n",
        "   255, 72, 6,\n",
        "   124, 9, 34,\n",
        "   193, 17, 15,\n",
        "   106, 17, 21,\n",
        "   28, 213, 2,\n",
        "   252, 155, 3,\n",
        "   20, 147, 77,\n",
        "   9, 83, 3,\n",
        "   11, 16, 138,\n",
        "   6, 12, 24,\n",
        "   20, 22, 37,\n",
        "   225, 14, 16,\n",
        "   23, 5, 29,\n",
        "   14, 12, 28,\n",
        "   15, 11, 16,\n",
        "   3, 12, 24,\n",
        "   22, 3, 11\n",
        "   ]\n",
        "\n",
        "cat_palette = [255,  255,  255,\n",
        "            220, 220, 0,\n",
        "           190, 153, 153,\n",
        "            250, 170, 30,\n",
        "           220, 220, 0,\n",
        "           107, 142, 35,\n",
        "           102, 102, 156,\n",
        "           152, 251, 152,\n",
        "           119, 11, 32,\n",
        "           244, 35, 232,\n",
        "           220, 20, 60,\n",
        "           52 , 83  ,84,\n",
        "          194 , 87 , 125,\n",
        "          225,  96  ,18,\n",
        "          31 , 102 , 211,\n",
        "          104 , 131 , 101\n",
        "          ]"
      ],
      "metadata": {
        "id": "ISDHS2P6rLhw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate_data"
      ],
      "metadata": {
        "id": "kZZStgYr02_1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MgRMSJnCqHNA"
      },
      "outputs": [],
      "source": [
        "def generate_data(args, checkpoint_path, num_sample, start_step=0, vis=True):\n",
        "    if args['category'] == 'car':\n",
        "        palette = car_20_palette\n",
        "    elif args['category'] == 'face':\n",
        "        palette = face_palette\n",
        "    elif args['category'] == 'bedroom':\n",
        "        palette = bedroom_palette\n",
        "    elif args['category'] == 'cat':\n",
        "        palette = cat_palette\n",
        "    else:\n",
        "        assert False\n",
        "    if not vis:\n",
        "        result_path = os.path.join(checkpoint_path, 'samples' )\n",
        "    else:\n",
        "        result_path = os.path.join(checkpoint_path, 'vis_%d'%num_sample)\n",
        "    if os.path.exists(result_path):\n",
        "        pass\n",
        "    else:\n",
        "        os.system('mkdir -p %s' % (result_path))\n",
        "        print('Experiment folder created at: %s' % (result_path))\n",
        "\n",
        "\n",
        "    g_all, avg_latent, upsamplers = prepare_stylegan(args)\n",
        "\n",
        "    classifier_list = []\n",
        "    for MODEL_NUMBER in range(args['model_num']):\n",
        "        print('MODEL_NUMBER', MODEL_NUMBER)\n",
        "\n",
        "        classifier = pixel_classifier(numpy_class=args['number_class']\n",
        "                                      , dim=args['dim'][-1])\n",
        "        classifier =  nn.DataParallel(classifier, device_ids=device_ids)\n",
        "        if torch.cuda.is_available():\n",
        "            classifier = classifier.cuda()\n",
        "\n",
        "        checkpoint = torch.load(os.path.join(checkpoint_path, 'model_' + str(MODEL_NUMBER) + '.pth'))\n",
        "\n",
        "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "        classifier.eval()\n",
        "        classifier_list.append(classifier)\n",
        "\n",
        "    softmax_f = nn.Softmax(dim=1)\n",
        "    with torch.no_grad():\n",
        "        latent_cache = []\n",
        "        image_cache = []\n",
        "        seg_cache = []\n",
        "        entropy_calculate = []\n",
        "        results = []\n",
        "        np.random.seed(start_step)\n",
        "        count_step = start_step\n",
        "\n",
        "\n",
        "\n",
        "        print( \"num_sample: \", num_sample)\n",
        "\n",
        "        for i in range(num_sample):\n",
        "            if i % 20 == 0:\n",
        "                print(\"Generate\", i, \"Out of:\", num_sample)\n",
        "\n",
        "            curr_result = {}\n",
        "\n",
        "            latent = np.random.randn(1, 512)\n",
        "\n",
        "            curr_result['latent'] = latent\n",
        "\n",
        "\n",
        "            latent = torch.from_numpy(latent).type(torch.FloatTensor).to(device)\n",
        "            latent_cache.append(latent)\n",
        "\n",
        "            img, affine_layers = latent_to_image(g_all, upsamplers, latent, dim=args['dim'][1],\n",
        "                                                     return_upsampled_layers=True)\n",
        "\n",
        "            if args['dim'][0] != args['dim'][1]:\n",
        "                img = img[:, 64:448][0]\n",
        "            else:\n",
        "                img = img[0]\n",
        "\n",
        "            image_cache.append(img)\n",
        "            if args['dim'][0] != args['dim'][1]:\n",
        "                affine_layers = affine_layers[:, :, 64:448]\n",
        "            affine_layers = affine_layers[0]\n",
        "\n",
        "            affine_layers = affine_layers.reshape(args['dim'][-1], -1).transpose(1, 0)\n",
        "\n",
        "            all_seg = []\n",
        "            all_entropy = []\n",
        "            mean_seg = None\n",
        "\n",
        "            seg_mode_ensemble = []\n",
        "            for MODEL_NUMBER in range(args['model_num']):\n",
        "                classifier = classifier_list[MODEL_NUMBER]\n",
        "\n",
        "                img_seg = classifier(affine_layers)\n",
        "\n",
        "                img_seg = img_seg.squeeze()\n",
        "\n",
        "\n",
        "                entropy = Categorical(logits=img_seg).entropy()\n",
        "                all_entropy.append(entropy)\n",
        "\n",
        "                all_seg.append(img_seg)\n",
        "                if mean_seg is None:\n",
        "                    mean_seg = softmax_f(img_seg)\n",
        "                else:\n",
        "                    mean_seg += softmax_f(img_seg)\n",
        "\n",
        "                img_seg_final = oht_to_scalar(img_seg)\n",
        "                img_seg_final = img_seg_final.reshape(args['dim'][0], args['dim'][1], 1)\n",
        "                img_seg_final = img_seg_final.cpu().detach().numpy()\n",
        "\n",
        "                seg_mode_ensemble.append(img_seg_final)\n",
        "\n",
        "            mean_seg = mean_seg / len(all_seg)\n",
        "\n",
        "            full_entropy = Categorical(mean_seg).entropy()\n",
        "\n",
        "            js = full_entropy - torch.mean(torch.stack(all_entropy), 0)\n",
        "\n",
        "            top_k = js.sort()[0][- int(js.shape[0] / 10):].mean()\n",
        "            entropy_calculate.append(top_k)\n",
        "\n",
        "\n",
        "            img_seg_final = np.concatenate(seg_mode_ensemble, axis=-1)\n",
        "            img_seg_final = scipy.stats.mode(img_seg_final, 2)[0].reshape(args['dim'][0], args['dim'][1])\n",
        "            del (affine_layers)\n",
        "            if vis:\n",
        "\n",
        "                color_mask = 0.7 * colorize_mask(img_seg_final, palette) + 0.3 * img\n",
        "\n",
        "                imageio.imwrite(os.path.join(result_path, \"vis_\" + str(i) + '.jpg'),\n",
        "                                  color_mask.astype(np.uint8))\n",
        "                imageio.imwrite(os.path.join(result_path, \"vis_\" + str(i) + '_image.jpg'),\n",
        "                                  img.astype(np.uint8))\n",
        "            else:\n",
        "                seg_cache.append(img_seg_final)\n",
        "                curr_result['uncertrainty_score'] = top_k.item()\n",
        "                image_label_name = os.path.join(result_path, 'label_' + str(count_step) + '.png')\n",
        "                image_name = os.path.join(result_path,  str(count_step) + '.png')\n",
        "\n",
        "                js_name = os.path.join(result_path, str(count_step) + '.npy')\n",
        "                img = Image.fromarray(img)\n",
        "                img_seg = Image.fromarray(img_seg_final.astype('uint8'))\n",
        "                js = js.cpu().numpy().reshape(args['dim'][0], args['dim'][1])\n",
        "                img.save(image_name)\n",
        "                img_seg.save(image_label_name)\n",
        "                np.save(js_name, js)\n",
        "                curr_result['image_name'] = image_name\n",
        "                curr_result['image_label_name'] = image_label_name\n",
        "                curr_result['js_name'] = js_name\n",
        "                count_step += 1\n",
        "\n",
        "\n",
        "                results.append(curr_result)\n",
        "                if i % 1000 == 0 and i != 0:\n",
        "                    with open(os.path.join(result_path, str(i) + \"_\" + str(start_step) + '.pickle'), 'wb') as f:\n",
        "                        pickle.dump(results, f)\n",
        "\n",
        "        with open(os.path.join(result_path, str(num_sample) + \"_\" + str(start_step) + '.pickle'), 'wb') as f:\n",
        "            pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare_data"
      ],
      "metadata": {
        "id": "RxRnx6740RJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EEWi2q8aqHNB"
      },
      "outputs": [],
      "source": [
        "def prepare_data(args, palette):\n",
        "    print('preparing data...')\n",
        "    g_all, avg_latent, upsamplers = prepare_stylegan(args)\n",
        "    print('prepared stylegan')\n",
        "    latent_all = np.load(args['annotation_image_latent_path'])\n",
        "    latent_all = torch.from_numpy(latent_all)\n",
        "    if torch.cuda.is_available():\n",
        "        latent_all = latent_all.cuda()\n",
        "\n",
        "    # load annotated mask\n",
        "    print('loading annotated mask')\n",
        "    mask_list = []\n",
        "    im_list = []\n",
        "    latent_all = latent_all[:args['max_training']]\n",
        "    num_data = len(latent_all)\n",
        "\n",
        "    for i in range(len(latent_all)):\n",
        "        print(i)\n",
        "\n",
        "        if i >= args['max_training']:\n",
        "            break\n",
        "        name = 'image_mask%0d.npy' % i\n",
        "\n",
        "        im_frame = np.load(os.path.join( args['annotation_mask_path'] , name))\n",
        "        mask = np.array(im_frame)\n",
        "        mask =  cv2.resize(np.squeeze(mask), dsize=(args['dim'][1], args['dim'][0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        mask_list.append(mask)\n",
        "\n",
        "        im_name = os.path.join( args['annotation_mask_path'], 'image_%d.jpg' % i)\n",
        "        img = Image.open(im_name)\n",
        "        img = img.resize((args['dim'][1], args['dim'][0]))\n",
        "\n",
        "        im_list.append(np.array(img))\n",
        "\n",
        "    # delete small annotation error\n",
        "    for i in range(len(mask_list)):  # clean up artifacts in the annotation, must do\n",
        "        for target in range(1, 50):\n",
        "            if (mask_list[i] == target).sum() < 30:\n",
        "                mask_list[i][mask_list[i] == target] = 0\n",
        "\n",
        "\n",
        "    all_mask = np.stack(mask_list)\n",
        "\n",
        "\n",
        "    # 3. Generate ALL training data for training pixel classifier\n",
        "    print('generating training data')\n",
        "    all_feature_maps_train = np.zeros((args['dim'][0] * args['dim'][1] * len(latent_all), args['dim'][2]), dtype=np.float16)\n",
        "    print('feature maps')\n",
        "    all_mask_train = np.zeros((args['dim'][0] * args['dim'][1] * len(latent_all),), dtype=np.float16)\n",
        "    print('masks')\n",
        "\n",
        "\n",
        "    vis = []\n",
        "    for i in range(len(latent_all) ):\n",
        "        print(i)\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        latent_input = latent_all[i].float()\n",
        "\n",
        "        img, feature_maps = latent_to_image(g_all, upsamplers, latent_input.unsqueeze(0), dim=args['dim'][1],\n",
        "                                            return_upsampled_layers=True, use_style_latents=args['annotation_data_from_w'])\n",
        "        \n",
        "        if args['dim'][0]  != args['dim'][1]:\n",
        "            # only for car\n",
        "            img = img[:, 64:448]\n",
        "            feature_maps = feature_maps[:, :, 64:448]\n",
        "        mask = all_mask[i:i + 1]\n",
        "        feature_maps = feature_maps.permute(0, 2, 3, 1)\n",
        "\n",
        "        feature_maps = feature_maps.reshape(-1, args['dim'][2])\n",
        "        new_mask =  np.squeeze(mask)\n",
        "\n",
        "        mask = mask.reshape(-1)\n",
        "\n",
        "        all_feature_maps_train[args['dim'][0] * args['dim'][1] * i: args['dim'][0] * args['dim'][1] * i + args['dim'][0] * args['dim'][1]] = feature_maps.cpu().detach().numpy().astype(np.float16)\n",
        "        all_mask_train[args['dim'][0] * args['dim'][1] * i:args['dim'][0] * args['dim'][1] * i + args['dim'][0] * args['dim'][1]] = mask.astype(np.float16)\n",
        "\n",
        "        img_show =  cv2.resize(np.squeeze(img[0]), dsize=(args['dim'][1], args['dim'][1]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        curr_vis = np.concatenate( [im_list[i], img_show, colorize_mask(new_mask, palette)], 0 )\n",
        "\n",
        "        vis.append( curr_vis )\n",
        "\n",
        "\n",
        "    vis = np.concatenate(vis, 1)\n",
        "\n",
        "    print('vis = np.concatenate(vis, 1)')\n",
        "    \n",
        "    imageio.imwrite(os.path.join(args['exp_dir'], \"train_data.jpg\"), vis)\n",
        "\n",
        "    print('imageio.imwrite(os.path.join(args[\\'exp_dir\\'], \\\"train_data.jpg\\\"), vis)')\n",
        "\n",
        "    return all_feature_maps_train, all_mask_train, num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main training"
      ],
      "metadata": {
        "id": "3XHj7plr0T-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZAP-6Tu4qHND"
      },
      "outputs": [],
      "source": [
        "def train_interpreter_main(args):\n",
        "\n",
        "    if args['category'] == 'car':\n",
        "        palette = car_20_palette\n",
        "    elif args['category'] == 'face':\n",
        "        palette = face_palette\n",
        "    elif args['category'] == 'bedroom':\n",
        "        palette = bedroom_palette\n",
        "    elif args['category'] == 'cat':\n",
        "        palette = cat_palette\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "    all_feature_maps_train_all, all_mask_train_all, num_data = prepare_data(args, palette)\n",
        "    print('data prepared')\n",
        "\n",
        "    torch.FloatTensor(all_feature_maps_train_all)\n",
        "    print('all_feature_maps_train_all float tensor success')\n",
        "    torch.FloatTensor(all_mask_train_all)\n",
        "    print('all_mask_train_all float tensor success')\n",
        "\n",
        "    train_data = trainData(torch.FloatTensor(all_feature_maps_train_all),\n",
        "                           torch.FloatTensor(all_mask_train_all))\n",
        "    \n",
        "    print('data floated')\n",
        "\n",
        "\n",
        "    count_dict = get_label_stas(train_data)\n",
        "\n",
        "    print('label stas got')\n",
        "\n",
        "    max_label = args['number_class'] - 1 #max([*count_dict])\n",
        "    \n",
        "    print(\" *********************** max_label \" + str(max_label) + \" ***********************\")\n",
        "\n",
        "\n",
        "    print(\" *********************** Current number data \" + str(num_data) + \" ***********************\")\n",
        "\n",
        "\n",
        "    batch_size = args['batch_size']\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(\" *********************** Current dataloader length \" +  str(len(train_loader)) + \" ***********************\")\n",
        "\n",
        "    for MODEL_NUMBER in range(args['model_num']):\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        classifier = pixel_classifier(numpy_class=(max_label + 1), dim=args['dim'][-1])\n",
        "\n",
        "        classifier.init_weights()\n",
        "\n",
        "        classifier = nn.DataParallel(classifier, device_ids=device_ids)\n",
        "        if torch.cuda.is_available():\n",
        "            classifier = classifier.cuda()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "        classifier.train()\n",
        "\n",
        "\n",
        "        iteration = 0\n",
        "        break_count = 0\n",
        "        best_loss = 10000000\n",
        "        stop_sign = 0\n",
        "        for epoch in range(100):\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                y_batch = y_batch.type(torch.long)\n",
        "                y_batch = y_batch.type(torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = classifier(X_batch)\n",
        "                loss = criterion(y_pred, y_batch)\n",
        "                acc = multi_acc(y_pred, y_batch)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                iteration += 1\n",
        "                if iteration % 1000 == 0:\n",
        "                    print('Epoch : ', str(epoch), 'iteration', iteration, 'loss', loss.item(), 'acc', acc)\n",
        "                    gc.collect()\n",
        "\n",
        "\n",
        "                # if iteration % 5000 == 0:\n",
        "                #     model_path = os.path.join(args['exp_dir'],\n",
        "                #                               'model_20parts_iter' +  str(iteration) + '_number_' + str(MODEL_NUMBER) + '.pth')\n",
        "                #     print('Save checkpoint, Epoch : ', str(epoch), ' Path: ', model_path)\n",
        "\n",
        "                #     torch.save({'model_state_dict': classifier.state_dict()},\n",
        "                #                model_path)\n",
        "\n",
        "                if epoch > 3:\n",
        "                    if loss.item() < best_loss:\n",
        "                        best_loss = loss.item()\n",
        "                        break_count = 0\n",
        "                    else:\n",
        "                        break_count += 1\n",
        "\n",
        "                    if break_count > 50:\n",
        "                        stop_sign = 1\n",
        "                        print(\"*************** Break, Total iters,\", iteration, \", at epoch\", str(epoch), \"***************\")\n",
        "                        break\n",
        "\n",
        "            if stop_sign == 1:\n",
        "                break\n",
        "\n",
        "        gc.collect()\n",
        "        model_path = os.path.join(args['exp_dir'],\n",
        "                                  'model_' + str(MODEL_NUMBER) + '.pth')\n",
        "        MODEL_NUMBER += 1\n",
        "        print('save to:',model_path)\n",
        "        torch.save({'model_state_dict': classifier.state_dict()},\n",
        "                   model_path)\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()    # clear cache memory on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8en_p4SAqHNE"
      },
      "source": [
        "## Run Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnNWBxMuqHNF"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "      'exp': 'baseline_cats/cat_16.json',\n",
        "      'exp_dir': '',\n",
        "      'generate_data': False,\n",
        "      'save_vis': False,\n",
        "      'start_step': 0,\n",
        "      'resume': '',\n",
        "      'num_sample': 1000\n",
        "  }\n",
        "\n",
        "\n",
        "opts = json.load(open(args['exp'], 'r'))\n",
        "print(\"Opt\", opts)\n",
        "\n",
        "if args['exp_dir'] != \"\":\n",
        "    opts['exp_dir'] = args['exp_dir']\n",
        "\n",
        "\n",
        "path =opts['exp_dir']\n",
        "if os.path.exists(path):\n",
        "  pass\n",
        "else:\n",
        "  os.system('mkdir -p %s' % (path))\n",
        "  print('Experiment folder created at: %s' % (path))\n",
        "\n",
        "os.system('cp %s %s' % (args['exp'], opts['exp_dir']))\n",
        "\n",
        "train_interpreter_main(opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r 'model_dir_cat_baselines.zip' 'model_dir'"
      ],
      "metadata": {
        "id": "I4sfbEyR--5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/model_dir_cat_baselines.zip"
      ],
      "metadata": {
        "id": "_0CIX9UG2JVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Generator"
      ],
      "metadata": {
        "id": "d7BU3uvVtgZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjh-03X7qHNH"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "      'exp': 'model_dir/cat_16/cat_16.json',\n",
        "      'exp_dir': '',\n",
        "      'generate_data': True,\n",
        "      'save_vis': False,\n",
        "      'start_step': 0,\n",
        "      'resume': f'model_dir/cat_16',\n",
        "      'num_sample': 1000\n",
        "  }\n",
        "\n",
        "\n",
        "opts = json.load(open(args['exp'], 'r'))\n",
        "print(\"Opt\", opts)\n",
        "\n",
        "if args['exp_dir'] != \"\":\n",
        "  opts['exp_dir'] = args['exp_dir']\n",
        "\n",
        "\n",
        "path =opts['exp_dir']\n",
        "if os.path.exists(path):\n",
        "      pass\n",
        "else:\n",
        "      os.system('mkdir -p %s' % (path))\n",
        "      print('Experiment folder created at: %s' % (path))\n",
        "\n",
        "os.system('cp %s %s' % (args['exp'], opts['exp_dir']))\n",
        "\n",
        "generate_data(opts, args['resume'], args['num_sample'], vis=args['save_vis'], start_step=args['start_step'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r 'model_dir_cat_baseline_withsamples.zip' 'model_dir'"
      ],
      "metadata": {
        "id": "QN9RM9FzZlJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dBTdg4l5ijzg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "generator_playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}